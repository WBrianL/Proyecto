# üöÄ Sistema de Procesamiento de Pedidos Tolerante a Fallos

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![Kafka](https://img.shields.io/badge/Kafka-231F20?style=for-the-badge&logo=apachekafka&logoColor=white)
![Kubernetes](https://img.shields.io/badge/Kubernetes-326CE5?style=for-the-badge&logo=kubernetes&logoColor=white)

## üìå Objetivo
Sistema distribuido para procesamiento de pedidos en l√≠nea con tolerancia integrada a fallos en:
- Microservicios
- Infraestructura de red
- Bases de datos
- Componentes cr√≠ticos

## üèóÔ∏è Arquitectura
```mermaid
graph TD
    F[Frontend] --> G[API Gateway]
    G --> O[Servicio de Pedidos]
    G --> I[Servicio de Inventario]
    O --> P[(PostgreSQL)]
    I --> R[(Redis)]
    O --> K[Apache Kafka]
    I --> K
    K --> M[Monitorizaci√≥n]
    M -->|Prometheus| GR[Grafana]
    P -->|Replicaci√≥n| P2[(PostgreSQL R√©plica)]
üõ°Ô∏è T√©cnicas de Tolerancia a Fallos
T√©cnica	Implementaci√≥n	Beneficio
Circuit Breaker	pybreaker con umbral de 3 fallos	Evita cascadas de fallos
Reintentos Exponenciales	tenacity (5 intentos max)	Manejo de fallos transitorios
Replicaci√≥n PostgreSQL	Streaming replication	Alta disponibilidad de datos
Colas Resilientes	Kafka con retenci√≥n 7 d√≠as	Cero p√©rdida de pedidos
Auto-Recuperaci√≥n	Kubernetes + LivenessProbes	MTTR < 2 minutos
üöÄ Despliegue
Requisitos
Docker 20.10+

Kubernetes 1.25+

Helm 3.10+ (opcional para monitorizaci√≥n)

1. Iniciar Infraestructura
bash
Copy
docker compose -f kafka-postgres.yml up -d
2. Desplegar Microservicios
bash
Copy
kubectl apply -f k8s-deployment.yml
3. Verificar Estado
bash
Copy
kubectl get pods -w
# O usando Lens/K9s
üîç Simulaci√≥n de Fallos
bash
Copy
# 1. Apagar r√©plica PostgreSQL
docker stop postgres-replica

# 2. Verificar failover autom√°tico
kubectl exec -it orders-service -- curl localhost:5000/health

# 3. Cortar tr√°fico a inventory-service
kubectl scale deploy inventory-service --replicas=0

# 4. Observar Circuit Breaker
watch -n 1 "kubectl logs orders-service | grep BREAKER"
üìä M√©tricas Clave
python
Copy
# Ejemplo de m√©tricas en Prometheus
AVAILABILITY = """
sum(rate(http_requests_total{status!~"5.."}[5m])) 
/ 
sum(rate(http_requests_total[5m]))
"""

MTTR = """
avg(time() - process_start_time_seconds{job=~".*service"} 
* on(pod) group_left 
kube_pod_container_status_last_terminated_reason{reason="Error"})
"""
üìÇ Estructura del Proyecto
Copy
.
‚îú‚îÄ‚îÄ /frontend                 # Interfaz de usuario
‚îú‚îÄ‚îÄ /api-gateway              # Enrutamiento (FastAPI)
‚îú‚îÄ‚îÄ /orders-service           # Procesamiento de pedidos
‚îÇ   ‚îú‚îÄ‚îÄ circuit_breaker.py    # L√≥gica de Circuit Breaker
‚îÇ   ‚îî‚îÄ‚îÄ kafka_producer.py     # Publicaci√≥n a Kafka
‚îú‚îÄ‚îÄ /inventory-service        # Gesti√≥n de inventario
‚îú‚îÄ‚îÄ /k8s-deployment           # Configs Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml              # Auto-escalado
‚îÇ   ‚îî‚îÄ‚îÄ probes.yaml           # Health Checks
‚îú‚îÄ‚îÄ docker-compose.yml        # Orquestaci√≥n local
‚îî‚îÄ‚îÄ chaos-test                # Pruebas de fallos
    ‚îî‚îÄ‚îÄ network-partition.py  # Simulador de fallos
üß™ Pruebas de Resiliencia
python
Copy
# Ejemplo test con pytest
def test_order_processing_resilience():
    # 1. Simular fallo en inventory-service
    kill_pod("inventory-service-1")
    
    # 2. Enviar pedido
    response = post_order(test_order)
    
    # 3. Verificar recuperaci√≥n
    assert response.status_code == 202  # Accepted
    assert kafka_message_exists("pending_orders")
üìà SLA Garantizado
M√©trica	Objetivo	Implementaci√≥n
Disponibilidad	99.95%	Replicaci√≥n + Auto-healing
MTTR	< 2 min	Kubernetes + Probes
P√©rdida de datos	0%	Kafka + Ack "all"
